import os
import requests
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm


def download_file(url, dest_folder):
    try:
        response = requests.get(url, timeout=10, stream=True)
        file_name = os.path.basename(url)

        if not os.path.exists(dest_folder):
            os.makedirs(dest_folder)

        with open(os.path.join(dest_folder, file_name), 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
    except Exception as e:
        print(f"Error downloading {url}: {e}")


def check_existing_files(urls, dest_folder):
    if os.path.exists(dest_folder):
        base_addres = urls[1].split('/')[:-1]
        base_address = "/".join(base_addres)

        existing_files = set(f for f in os.listdir(dest_folder))

        target_files = set([urls.split('/')[-1].split('?')[0]
                           for urls in urls])

        urls = [base_address + '/' + f
                for f in target_files - existing_files]
        print(f"Found {len(existing_files)} existing files.")

    return urls


def main(input_file, num_threads=10):
    dest_folder = os.path.splitext(input_file)[0]

    with open(input_file) as f:
        urls = [line.strip() for line in f.readlines()]

    urls = check_existing_files(urls, dest_folder)

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(
            download_file, url, dest_folder) for url in urls]
        for future in tqdm(futures, desc="Downloading files"):
            future.result()

    print("Download complete.")


if __name__ == "__main__":
    num_threads = 10
    for i in range(3, 8):
        main(rf"Scrapper\sms_links.txt_part{i}.txt", num_threads)

