import os
import tarfile
import shutil
from shutil import make_archive
from tqdm import tqdm
import concurrent.futures


def extract_single_file(file, source_folder, processed_folder, report_file):
    '''
    Extracts a single file from a tar.gz archive and writes it to a JSON file.

    Args:
        file (str): The name of the tar.gz archive file to process.
        source_folder (str): The path to the folder containing the tar.gz archive file.
        processed_folder (str): The path to the folder where the JSON file should be saved.
        report_file (str): The path to the file where the processing report should be saved.

    Returns:
        str: A message indicating whether the file was processed successfully or not.
    '''
    try:
        # Open the tar.gz archive file and search for the JSON file
        tar_file_path = os.path.join(source_folder, file)
        with tarfile.open(tar_file_path, 'r:gz', errors="ignore") as tar:
            json_file_name = 'sample_for_analysis.apk.static.json'
            json_file = None

            for tarinfo in tar:
                if tarinfo.name.endswith(json_file_name):
                    json_file = tar.extractfile(tarinfo)
                    break

            # If the JSON file was found, save it to the processed folder
            if json_file:
                dest_folder = os.path.join(
                    processed_folder, os.path.splitext(file)[0] + '.json')
                with open(dest_folder, 'wb') as dest_file:
                    dest_file.write(json_file.read())
                return f"Processed: {file}\n"
            # If the JSON file was not found, report the error
            else:
                return f"Failed: {file} - Error: {json_file_name} not found\n"

    # If there was an error reading the tar.gz archive file, report the error
    except tarfile.ReadError as e:
        return f"Failed: {file} - Error: {e}\n"
    # If there was any other error, report the error
    except Exception as e:
        return f"Failed: {file} - Error: {e}\n"


def make_archive(source, destination, format='zip', input_name=None):
    '''
    Archives a folder and moves the archive file to a specified destination.

    Args:
        source (str): The path to the folder to archive.
        destination (str): The path to the destination folder where the archive file should be moved.
        format (str): The format to use for the archive file (default is 'zip').
        input_name (str): The name to use for the archive file (default is None).
    '''
    if not os.path.exists(destination):
        os.makedirs(destination)

    # Move the processing report to the processed folder
    new_report_path = os.path.join(
        processed_folder, os.path.basename(report_file))
    shutil.move(report_file, new_report_path)

    # Archive the processed folder and move the archive file to the destination folder
    name = f"Processed_{input_name}"
    archive_from = os.path.dirname(source)
    archive_to = os.path.basename(source.strip(os.sep))
    print(
        f'Source: {source}\nDestination: {destination}\n')
    shutil.make_archive(name, format, archive_from, archive_to)
    shutil.move('%s.%s' % (name, format), destination)


def sort_report_alphabetically(report_file):
    '''
    Sorts the lines in a text file alphabetically.

    Args:
        report_file (str): The path to the text file to sort.
    '''
    with open(report_file, 'r') as infile:
        lines = infile.readlines()

    # Sort the lines alphabetically
    sorted_lines = sorted(lines, key=str.lower)

    # Write the sorted lines back to the file
    with open(report_file, 'w') as outfile:
        for line in sorted_lines:
            outfile.write(line)


def extract_tar_gz_files(source_folder, processed_folder, report_file):
    '''
    Extracts all tar.gz archive files in a folder and saves their contents to JSON files.

    Args:
        source_folder (str): The path to the folder containing the tar.gz archive files.
        processed_folder (str): The path to the folder where the JSON files should be saved.
        report_file (str): The path to the file where the processing report should be saved.
    '''
    tar_gz_files = []
    for root, dirs, files in os.walk(source_folder):
        for file in files:
            if file.endswith('.tar.gz'):
                tar_gz_files.append(file)

    # Process the tar.gz archive files concurrently
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = list(tqdm(executor.map(extract_single_file, tar_gz_files, [source_folder] * len(tar_gz_files), [
                       processed_folder] * len(tar_gz_files), [report_file] * len(tar_gz_files)), total=len(tar_gz_files)))

    # Write the processing report to a file
    with open(report_file, 'w') as report:
        for result in results:
            report.write(result)


# Process the downloaded banking data files
for i in range(5, 6):
    source_folder = rf'Scrapper\downloaded\banking_part{i}'
    name = source_folder.split("\\")[-1]
    print(f"Processing {name}...")
    processed_folder = os.path.join(source_folder, 'Processed')
    report_file = os.path.join(source_folder, f'{name}_report.txt')

    # Create the processed folder if it does not exist
    if not os.path.exists(processed_folder):
        os.makedirs(processed_folder)

    # Extract the tar.gz archive files and save their contents to JSON files
    extract_tar_gz_files(source_folder, processed_folder, report_file)

    # Sort the processing report alphabetically
    sort_report_alphabetically(report_file)

    # Archive the processed folder and move the archive file to the downloaded folder
    make_archive(processed_folder, 'Scrapper\downloaded', input_name=name)
