import os
import re
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urljoin


def download_file(url, local_filename):
    os.makedirs("Scrapper\\benign", exist_ok=True)
    local_filename = os.path.join("Scrapper\\benign", local_filename)
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return local_filename


def process_tar_gz(url):
    file_name = url.split("/")[-1]
    if not os.path.isfile(os.path.join("Scrapper\\benign", file_name)):
        download_file(url, file_name)
    return file_name


def process_dir(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    file_name = None
    for link in soup.find_all('a', href=True):
        if link['href'].endswith("sample_for_analysis.apk.static.json"):
            file_name = url.split("/")[-2] + ".json"
            if not os.path.isfile(os.path.join("Scrapper\\benign", file_name)):
                download_file(urljoin(url, link['href']), file_name)
            break
    return file_name


def process_target(target_url):
    try:
        if target_url.endswith(".tar.gz"):
            return ("tar", process_tar_gz(target_url))
        else:
            return ("dir", process_dir(target_url))
    except Exception as e:
        return ("error", f"{target_url} - Error: {e}")


def main(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    target_urls = []
    for link in soup.find_all('a', href=True):
        if re.match(r"^[0-9A-F]{64}(/|.tar.gz)$", link['href']):
            target_urls.append(urljoin(url, link['href']))

    with ThreadPoolExecutor() as executor:
        results = list(executor.map(process_target, target_urls))

    os.makedirs("Scrapper\\benign", exist_ok=True)
    with open(os.path.join("Scrapper\\benign", "benign_report.txt"), "w") as f:
        for res_type, res_name in results:
            if res_type == "error":
                f.write(f"Error: {res_name}\n")
            else:
                f.write(f"{res_type}: {res_name}\n")


def extract_patterns(input_file, output_file, pattern):
    with open(input_file, 'r') as infile:
        content = infile.read()

    matches = re.findall(pattern, content)

    with open(output_file, 'w') as outfile:
        for match in matches:
            outfile.write(match + '\n')


if __name__ == "__main__":
    # url = input("Enter the URL: ")
    # main(url)
    
    input_file = os.path.join("Scrapper\\benign", "benign_report.txt")
    output_file = os.path.join("Scrapper\Reports", "match.txt")
    original = os.path.join("Scrapper\Reports", "match2.txt")
    print(os.path.exists(input_file))
    pattern = r'[A-Fa-f0-9]{64}'

    extract_patterns(input_file, output_file, pattern)
    with open(output_file, 'r') as f:
        main_set=set(f.read().splitlines())
    with open(original, 'r') as f:
        compare_set=set(f.read().splitlines())

    print(compare_set-main_set)
