import os
import re
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urljoin


def download_file(url, local_filename):
    '''
    Downloads a file from the given URL and saves it to the specified local filename.

    Args:
        url (str): The URL to download the file from.
        local_filename (str): The local filename to save the downloaded file to.

    Returns:
        The local filename if the file download is successful.
    '''
    os.makedirs("Scrapper\\benign", exist_ok=True)
    local_filename = os.path.join("Scrapper\\benign", local_filename)
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb', encoding="utf-8") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return local_filename


def process_tar_gz(url):
    '''
    Processes a URL pointing to a .tar.gz file and downloads it if it does not already exist.

    Args:
        url (str): The URL pointing to the .tar.gz file.

    Returns:
        The name of the downloaded file if the download is successful, otherwise None.
    '''
    file_name = url.split("/")[-1]
    if not os.path.isfile(os.path.join("Scrapper\\benign", file_name)):
        download_file(url, file_name)
    return file_name


def process_dir(url):
    '''
    Processes a URL pointing to a directory and downloads the sample_for_analysis.apk.static.json file if it exists.

    Args:
        url (str): The URL pointing to the directory.

    Returns:
        The name of the downloaded file if the download is successful, otherwise None.
    '''
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    file_name = None
    for link in soup.find_all('a', href=True):
        if link['href'].endswith("sample_for_analysis.apk.static.json"):
            file_name = url.split("/")[-2] + ".json"
            if not os.path.isfile(os.path.join("Scrapper\\benign", file_name)):
                download_file(urljoin(url, link['href']), file_name)
            break
    return file_name


def process_target(target_url):
    '''
    Processes a target URL by determining its type and calling the appropriate function.

    Args:
        target_url (str): The URL of the target to process.

    Returns:
        A tuple containing the type of the processed target ("tar", "dir", or "error") and the name of the downloaded file if applicable.
    '''
    try:
        if target_url.endswith(".tar.gz"):
            return ("tar", process_tar_gz(target_url))
        else:
            return ("dir", process_dir(target_url))
    except Exception as e:
        return ("error", f"{target_url} - Error: {e}")


def main(url):
    '''
    Downloads and processes files from a URL pointing to a directory of benign samples.

    Args:
        url (str): The URL pointing to the directory of benign samples.
    '''
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    target_urls = []
    for link in soup.find_all('a', href=True):
        if re.match(r"^[0-9A-F]{64}(/|.tar.gz)$", link['href']):
            target_urls.append(urljoin(url, link['href']))

    with ThreadPoolExecutor() as executor:
        results = list(executor.map(process_target, target_urls))

    # Write the results to a text file
    os.makedirs("Scrapper\\benign", exist_ok=True)
    with open(os.path.join("Scrapper\\benign", "benign_report.txt"), "w") as f:
        for res_type, res_name in results:
            if res_type == "error":
                f.write(f"Error: {res_name}\n")
            else:
                f.write(f"{res_type}: {res_name}\n")


def extract_patterns(input_file, output_file, pattern):
    '''
    Extracts patterns from a text file and writes them to another text file.

    Args:
        input_file (str): The path to the input text file.
        output_file (str): The path to the output text file.
        pattern (str): The regular expression pattern to search for in the input file.
    '''
    with open(input_file, 'r', encoding="utf-8") as infile:
        content = infile.read()

    matches = re.findall(pattern, content)

    with open(output_file, 'w', encoding="utf-8") as outfile:
        for match in matches:
            outfile.write(match + '\n')


if __name__ == "__main__":
    # Call the main function with the specified URL
    url = input("Enter the URL: ")
    main(url)
    # Extract patterns from the benign report and compare with previous matches
    input_file = os.path.join("Scrapper\\benign", "benign_report.txt")
    output_file = os.path.join("Scrapper\\Reports", "match.txt")
    original = os.path.join("Scrapper\\Reports", "match2.txt")

    # Extract patterns matching the specified regular expression from the input file
    pattern = r'[A-Fa-f0-9]{64}'
    extract_patterns(input_file, output_file, pattern)

    # Compare the extracted patterns with those in the original match file
    with open(output_file, 'r', encoding="utf-8") as f:
        main_set = set(f.read().splitlines())
    with open(original, 'r', encoding="utf-8") as f:
        compare_set = set(f.read().splitlines())

    # Print any new matches that were not in the original match file
    print(compare_set-main_set)
