import os
import requests
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm


def download_file(url, dest_folder, pbar):
    '''
    Downloads a file from the given URL and saves it to the specified destination folder.

    Args:
        url (str): The URL to download the file from.
        dest_folder (str): The folder to save the downloaded file to.
        pbar (tqdm): A progress bar instance to update as the file download progresses.

    Returns:
        None if the file download is successful, otherwise the name of the failed file.
    '''
    try:
        # Send a GET request to the URL and stream the response
        response = requests.get(url, timeout=10, stream=True)

        # Extract the filename from the URL
        file_name = os.path.basename(url)

        # Create the destination folder if it doesn't exist
        if not os.path.exists(dest_folder):
            os.makedirs(dest_folder)

        # Open a file in binary write mode and write the streamed content to it in chunks
        with open(os.path.join(dest_folder, file_name), 'wb', encoding="utf-8") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        # Update the progress bar
        pbar.update(1)

        return None

    except Exception as e:
        # Print an error message if the file download fails and update the progress bar
        print(f"Error downloading {url}: {e}")
        pbar.update(1)
        return file_name


def check_existing_files(urls, dest_folder):
    '''
    Checks for existing files in the destination folder and returns a filtered list of URLs.

    Args:
        urls (list): A list of URLs to download files from.
        dest_folder (str): The folder where files will be saved.

    Returns:
        A list of URLs that correspond to files that do not already exist in the destination folder.
    '''
    if os.path.exists(dest_folder):
        # Extract the base address from the first URL
        base_address = "/".join(urls[1].split('/')[:-1])

        # Get the set of existing file names in the destination folder
        existing_files = set(f for f in os.listdir(dest_folder))

        # Get the set of target file names from the URLs
        target_files = set([url.split('/')[-1].split('?')[0] for url in urls])

        # Create a filtered list of URLs that correspond to files that don't exist in the destination folder
        urls = [base_address + '/' + f for f in target_files - existing_files]

        print(f"Found {len(existing_files)} existing files.")
        print(f"Downloading {len(urls)} new files.")

    return urls


def main(input_file, num_threads=10):
    '''
    Downloads files from a list of URLs specified in the input file.

    Args:
        input_file (str): The path to a text file containing a list of URLs to download files from.
        num_threads (int): The number of threads to use for concurrent file downloads. Default is 10.
    '''
    if not os.path.exists("Scrapper\\downloaded"):
        os.makedirs("Scrapper\\downloaded")

    # Create the destination folder using the input file name
    dest_folder = os.path.join(
        "Scrapper\\downloaded", os.path.splitext(input_file)[0].split('\\')[-1])

    print(f"Downloading {input_file} to {dest_folder}...")

    # Read the URLs from the input file
    with open(input_file) as f:
        urls = [line.strip() for line in f.readlines()]

    urls = check_existing_files(urls, dest_folder)

    # Initialize an empty list to store the names of any failed files
    failed_files = []

    # Create a thread pool executor and submit download tasks for each URL
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        with tqdm(total=len(urls), desc="Downloading files") as pbar:
            futures = [executor.submit(
                download_file, url, dest_folder, pbar) for url in urls]

            # Wait for each download task to complete and add the name of any failed file to the failed_files list
            for future in futures:
                failed_file = future.result()
                if failed_file:
                    failed_files.append(failed_file)

    print("Download complete.")

    # Delete any failed files from the destination folder
    for failed_file in failed_files:
        file_path = os.path.join(dest_folder, failed_file)
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"Deleted failed file: {failed_file}")


if __name__ == "__main__":
    # Run the main function with the specified input file and number of threads
    num_threads = 15
    for i in range(1, 6):
        main(rf"Scrapper\banking_part{i}.txt", num_threads)
